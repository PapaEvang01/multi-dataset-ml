Breast Cancer Classification — Phase 3B.2
Local Explainability (Hard Cases)

Overview
--------
This phase shifts the analysis from global feature importance to local,
sample-level explanations. Instead of asking which features matter on average,
we investigate why the model made a specific decision for individual,
challenging samples identified in Phase 3A.

The analysis focuses on:
- False Positives: benign samples predicted as malignant
- Borderline cases: samples with predicted probabilities close to the decision threshold

Goal
----
The objective is to understand which individual features pushed a single
prediction toward malignancy or pulled it toward benign. This transforms model
outputs into interpretable, case-specific explanations suitable for research
and medical decision-support contexts.

Method: Permutation-Based Local Influence
-----------------------------------------
For each selected test sample, a local explanation is computed using a
permutation-style approach:

1. Compute the baseline probability P(malignant) for the sample.
2. For each feature, repeatedly replace its value with a random value drawn
   from the training set (reference distribution).
3. Measure the change in P(malignant) relative to the baseline.
4. Average the change across multiple repetitions to estimate the feature’s
   local influence for that specific sample.

Interpretation
--------------
- Positive contribution: the feature increases malignancy probability
  (pushes the prediction toward malignant).
- Negative contribution: the feature decreases malignancy probability
  (pushes the prediction toward benign).

Features are ranked by the magnitude of their influence, highlighting the
most decisive factors for each individual prediction.

Reproducibility
---------------
- Reuses Phase 2 model definitions (no duplication of training logic)
- Uses the same stratified 70/30 train–test split as Phase 3A (random_state=42)
- Explains only real hard cases exported from Phase 3A CSV files

Outputs
-------
For each model (SVM_RBF and RandomForest), local explanation files are saved under:

results_phase3/explainability/<MODEL_NAME>/local_explanations/

Each file contains:
- Case type (false positive or borderline)
- Test-set index
- Baseline P(malignant)
- Top 10 contributing features with signed influence values

Why This Matters
----------------
In medical machine learning, performance metrics alone are insufficient.
Local explainability enables inspection of individual decisions, clarifies
failure modes (especially false positives), and builds trust by showing
which features drove a prediction on a case-by-case basis.
